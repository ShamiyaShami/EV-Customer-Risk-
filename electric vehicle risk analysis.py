# -*- coding: utf-8 -*-
"""Thesis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z9OdyRmKi7ff71Jb0XdQvERNu7dx6oPu
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

dataset= pd.read_csv("/content/drive/MyDrive/thesis/ev-reviews-dataset.csv", encoding= 'latin-1')

dataset.info()

dataset

Exact_Dataset= dataset[["Review","rating","Model"]]

Exact_Dataset

Exact_Dataset.duplicated()
Exact_Dataset.isnull().any().any()

Cleaned_Dataset=Exact_Dataset.dropna()

Cleaned_Dataset.isnull().any().any()

Cleaned_Dataset.duplicated()

Cleaned_Dataset.shape

x=Cleaned_Dataset['rating']
ax=sn.countplot(x)
ax.set(title="Distribution of Product Ratings", \
       xlabel="Rating", ylabel="Number of Reviews")
plt.show()

x.value_counts()

fig = plt.figure(figsize=(20,40))
ax=sn.countplot(y='Model', data=Cleaned_Dataset)
ax.set(title="Different of Product Models", \
       xlabel="Total", ylabel="Model")
plt.show()

Cleaned_Dataset['Model'].value_counts()

fig = plt.figure(figsize=(20,10))
ax=sn.lineplot(x=x,y='Model',data=Cleaned_Dataset)
ax.set(title="Distribution of Product Ratings", \
       xlabel="Rating", ylabel="Number of Reviews")
plt.show()

fig = plt.figure(figsize=(10,20))
tags = Cleaned_Dataset['rating'].value_counts()
tags.plot(kind='pie', autopct="%1.1f%%", label='')
plt.title("Distribution of the different ratings")
plt.show()

import seaborn as sns

f, axes = plt.subplots(figsize=(20,7))
ax = sns.countplot(x=Cleaned_Dataset["rating"], palette="OrRd_r")
ax.set(title="Distribution of Product Ratings", \
       xlabel="Rating", ylabel="Number of Reviews")
plt.show()

for i in range(5):
    print(Cleaned_Dataset['Review'].iloc[i], "\n")
    print(Cleaned_Dataset['rating'].iloc[i], "\n")

sample_review = Cleaned_Dataset["Review"].iloc[:]
print(sample_review)

import html
decoded_review = html.unescape(sample_review)
print(decoded_review)

pattern = r"\&\#[0-9]+\;"
Cleaned_Dataset["preprocessed"] = Cleaned_Dataset["Review"].str.replace(pat=pattern, repl="", regex=True)
print(Cleaned_Dataset["preprocessed"].iloc[:])

def data_processing(text):
    text = text.lower()
    text = re.sub(r"https\S+|www\S+|http\S+", '', text, flags = re.MULTILINE)
    text = re.sub(r'[^\w\s]','', text)
    text_tokens = word_tokenize(text)
    filtered_text = [w for w in text_tokens if not w in stop_words]
    return " ".join(filtered_text)

Cleaned_Dataset.preprocessed = Cleaned_Dataset['preprocessed'].apply(data_processing)

stemmer = PorterStemmer()
def stemming(data):
    text = [stemmer.stem(word) for word in data]
    return data

Cleaned_Dataset.Review = Cleaned_Dataset['Review'].apply(lambda x: stemming(x))
Cleaned_Dataset

for i in range(5):
    print(Cleaned_Dataset['Review'].iloc[i], "\n")
    print(Cleaned_Dataset['rating'].iloc[i], "\n")

# neg=Cleaned_Dataset.loc[Cleaned_Dataset["rating"]<5]
neg_reviews=Cleaned_Dataset[Cleaned_Dataset.rating<3]
neg_reviews

pos_reviews=Cleaned_Dataset[Cleaned_Dataset.rating>4]
pos_reviews

text = ' '.join([word for word in pos_reviews['Review']])
plt.figure(figsize=(20,15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in positive reviews', fontsize=19)
plt.show()

text = ' '.join([word for word in neg_reviews['Review']])
plt.figure(figsize=(20,15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in Negative reviews', fontsize=19)
plt.show()

from wordcloud import WordCloud

wordcloud = WordCloud(stopwords = set(stop_words), min_font_size=10, \
                      max_font_size=50, max_words=50, \
                      background_color="white")

one_star_text = " ".join(Cleaned_Dataset[Cleaned_Dataset["rating"]==1]["Review"].values).lower()
two_star_text = " ".join(Cleaned_Dataset[Cleaned_Dataset["rating"]==2]["Review"].values).lower()
three_star_text = " ".join(Cleaned_Dataset[Cleaned_Dataset["rating"]==3]["Review"].values).lower()
four_star_text = " ".join(Cleaned_Dataset[Cleaned_Dataset["rating"]==4]["Review"].values).lower()
five_star_text = " ".join(Cleaned_Dataset[Cleaned_Dataset["rating"]==5]["Review"].values).lower()

text_list = [one_star_text, two_star_text, three_star_text, \
             four_star_text, five_star_text]

for index, text in enumerate(text_list):
    f, axes = plt.subplots(figsize=(10,7))
    wordcloud.generate(text)
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.title(f"Word Cloud for {index+1}-Star Ratings")
    plt.axis("off")
    plt.show()

len(Cleaned_Dataset)

train_df=Cleaned_Dataset.iloc[:1300]
test_df=Cleaned_Dataset.iloc[1300:]

# hyperparameters
MAX_LEN = 256
TRAIN_BATCH_SIZE = 32
VALID_BATCH_SIZE = 32
EPOCHS = 2
LEARNING_RATE = 1e-05

!pip install transformers
!pip install tqdm
from tqdm import tqdm
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

token = tokenizer.encode_plus(
    Cleaned_Dataset['Review'].iloc[0], 
    max_length=256, 
    truncation=True, 
    padding='max_length', 
    add_special_tokens=True,
    return_tensors='tf'
)

token.input_ids

X_input_ids = np.zeros((len(Cleaned_Dataset), 256))
X_attn_masks = np.zeros((len(Cleaned_Dataset), 256))

def generate_training_data(Cleaned_Dataset, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(Cleaned_Dataset['Review'])):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=256, 
            truncation=True, 
            padding='max_length', 
            add_special_tokens=True,
            return_tensors='tf'
        )
        ids[i, :] = tokenized_text.input_ids
        masks[i, :] = tokenized_text.attention_mask
    return ids, masks

X_input_ids, X_attn_masks = generate_training_data(Cleaned_Dataset, X_input_ids, X_attn_masks, tokenizer)

labels = np.zeros((len(Cleaned_Dataset), 5))
labels.shape

import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))

def SentimentDatasetMapFunction(input_ids, attn_masks, labels):
    return {
        'input_ids': input_ids,
        'attention_mask': attn_masks
    }, labels

dataset = dataset.map(SentimentDatasetMapFunction)

dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)

p = 0.8
train_size = int((len(Cleaned_Dataset)//16)*p)

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size)

from transformers import TFBertModel

model = TFBertModel.from_pretrained('bert-base-cased')

input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)
output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes

sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
sentiment_model.summary()

tf.keras.utils.plot_model(sentiment_model, show_shapes=True, dpi=48)

optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)
loss_func = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])

sentiment_model.save('sentiment_model')

sentiment_model = tf.keras.models.load_model('sentiment_model')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

def prepare_data(input_text, tokenizer):
    token = tokenizer.encode_plus(
        input_text,
        max_length=256, 
        truncation=True, 
        padding='max_length', 
        add_special_tokens=True,
        return_tensors='tf'
    )
    return {
        'input_ids': tf.cast(token.input_ids, tf.float64),
        'attention_mask': tf.cast(token.attention_mask, tf.float64)
    }

def make_prediction(model, processed_data, classes=['Negative', 'A bit negative', 'Neutral', 'A bit positive', 'Positive']):
    probs = model.predict(processed_data)[0]
    return classes[np.argmax(probs)]

input_text = input('Enter user review here: ')
processed_data = prepare_data(input_text, tokenizer)
result = make_prediction(sentiment_model, processed_data=processed_data)
print(f"Predicted Issue: {result}")

from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D

tokenizer = Tokenizer(num_words=500, split=' ')
tokenizer.fit_on_texts(Cleaned_Dataset['Review'])
X = tokenizer.texts_to_sequences(Cleaned_Dataset['Review'])
X = pad_sequences(X)
X
# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# X = sc.fit_transform(X)

word_index = tokenizer.word_index
len(word_index)

X.shape

Y = Cleaned_Dataset['rating']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

model = Sequential()
model.add(Embedding(500, 120, input_length= X.shape[1]))
# model.add(SpatialDropout1D(0.4))
model.add(LSTM(128,activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu', return_sequences=False))
model.add(Dense(1))
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
print(model.summary())

tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)

history=model.fit(x_train, y_train, epochs=10, batch_size=32)

y_pred=model.predict(y_test)

Scores = model.evaluate(x_test, y_test)
print("Accuracy = %0.3f%%"%(Scores[1]*100))

